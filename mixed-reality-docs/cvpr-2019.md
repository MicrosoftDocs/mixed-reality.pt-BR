---
title: Pesquisa Visual Computacional aplicativos para a realidade misturada Workshop de headsets em CVPR 2019
description: Visão geral e programação dos aplicativos Pesquisa Visual Computacional para o workshop sobre headsets de realidade misturada, a ser entregue na conferência CVPR em junho de 2019.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: evento, modo de pesquisa, cvpr, visão computacional, pesquisa, HoloLens
ms.openlocfilehash: 89d79bcef77043564e51faada940d2c71a6005e4
ms.sourcegitcommit: 2f600e5ad00cd447b180b0f89192b4b9d86bbc7e
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 06/15/2019
ms.locfileid: "67148707"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a>Pesquisa Visual Computacional aplicativos para headsets de realidade misturada

Organizado em conjunto com [CVPR 2019](http://cvpr2019.thecvf.com/)

Praia (AC) longa

17 de junho de 2019 (à tarde) – Hyatt Regency F


## <a name="organizers"></a>Organizadores
* Pollefeys de hipermarca
* Federica Bogo
* Johannes Schönberger
* Osman Ulusoy

## <a name="overview"></a>Visão geral

![Imagem do separador](images/cvpr2019_teaser2.jpg)

Os headsets de realidade misturada, como o Microsoft HoloLens, estão se tornando plataformas poderosas para desenvolver aplicativos de pesquisa Visual computacional. O modo de pesquisa do HoloLens permite a pesquisa visual do computador no dispositivo, fornecendo acesso a todos os fluxos do sensor de imagem bruta, incluindo profundidade e IR. Como o modo de pesquisa agora está disponível desde 2018 de maio, estamos começando a ver várias demonstrações e aplicativos interessantes que estão sendo desenvolvidos para o HoloLens. 

O objetivo deste workshop é reunir alunos e pesquisadores interessados na visão computacional de aplicativos de realidade misturada. O workshop fornecerá um local para compartilhar demonstrações e aplicativos e aprenderá uns com os outros para criar ou portar aplicativos para a realidade misturada. 

Incentivamos envios sobre os tópicos de reconhecimento de objetos (ego), mão e rastreamento de usuários, reconhecimento de atividades, térrea, reconstrução de 3D, compreensão de cena, localização baseada em sensor, navegação e muito mais.

## <a name="paper-submission"></a>Envio de papel
* Prazo de envio do papel: 17 de maio
* Notificação aos autores: 24 de maio

Os envios de papel devem usar o modelo CVPR e são limitados a 4 páginas mais referências. Além disso, incentivamos os autores a enviar um vídeo mostrando seus aplicativos.
Observe que os envios de trabalhos publicados anteriormente são permitidos (incluindo o trabalho aceito para a conferência principal do CVPR 2019). 

Os envios podem ser carregados para o CMT: https://cmt3.research.microsoft.com/CVFORMR2019

Um subconjunto de documentos será selecionado para apresentação oral no workshop. No entanto, recomendamos enfaticamente que todos os autores apresentem seu trabalho durante a sessão de demonstração.


## <a name="schedule"></a>Agendamento
* 13:30-13:45: Bem-vindo e abrindo comentários.
* 13:45-14:15: **Palestra temática**: Prof. Marc Pollefeys, ETH Zurique/Microsoft. título Egocentric Pesquisa Visual Computacional no HoloLens.
* 14:15-14:45: **Palestra temática**: Prof. Kris Kitani, Carnegie Mellon University. título Atividade egocentric e gerar previsão.
* 14:45-15:15: **Palestra temática**: Recovery. Yang Liu, Instituto de tecnologia da Califórnia. título Capacitando um assistente cognitiva para o cego com realidade aumentada.
* 15:15-16:15: Intervalo de café e demonstrações.
* 16:15-16:45: **Palestra temática**: Prof. Kristen Grauman, Universidade do Texas em Austin/Facebook AI Research. título Interação de objeto humano no vídeo da primeira pessoa.
* 16:45-17:15: Apresentações verbais:
    * O registro fez navegação orthopedic fácil e autônoma com o HoloLens. FIXO. Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.
    * Aprendendo o estéreo ao percorrer um HoloLens. T. Zhan, Y. Pekelny, O. Ulusoy.
* 17:15-17:30: Comentários finais.
